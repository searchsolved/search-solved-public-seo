{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SEJ - Semantic Clustering Tool by @LeeFootSEO.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNPg1KRDiqGE5SruybV9zwL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/searchsolved/search-solved-public-seo/blob/main/search_engine_journal/SEJ_Semantic_Clustering_Tool_by_LeeFootSEO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Keyword Cluster Tool V1\n",
        "by [LeeFootSEO](https://twitter.com/LeeFootSEO) February 2022\n",
        "\n",
        "**Website:** https://www.searchsolved.co.uk\n",
        "\n",
        "**Twitter:** https://twitter.com/LeeFootSEO"
      ],
      "metadata": {
        "id": "MXHRpn-EYrbv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4p40j7Xng5O"
      },
      "outputs": [],
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install pandas\n",
        "!pip install chardet\n",
        "!pip install detect_delimiter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import time\n",
        "import sys\n",
        "import pandas as pd\n",
        "import chardet\n",
        "import codecs\n",
        "from detect_delimiter import detect\n",
        "\n",
        "from google.colab import files\n",
        "from sentence_transformers import SentenceTransformer, util"
      ],
      "metadata": {
        "id": "uzalzYeXntsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# File Types\n",
        "\n",
        "*   Script expects a column called 'Keyword'\n",
        "*   Recommend No More Than 50K Rows"
      ],
      "metadata": {
        "id": "GWR_3mfP2mNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the keyword export\n",
        "upload = files.upload()\n",
        "input_file = list(upload.keys())[0]  # get the name of the uploaded file"
      ],
      "metadata": {
        "id": "s9LF-cuyoPOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set Cluster Accuracy & Minimum Cluster Size"
      ],
      "metadata": {
        "id": "OkIJ-m9lmn_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_accuracy = 85  # 0-100 (100 = very tight clusters, but higher percentage of no_cluster groups)\n",
        "min_cluster_size = 2  # set the minimum size of cluster groups. (Lower number = tighter groups)"
      ],
      "metadata": {
        "id": "7uleqF6fTVMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choose a Sentence Transformer\n",
        "Download Pre-Trained Models: https://www.sbert.net/docs/pretrained_models.html"
      ],
      "metadata": {
        "id": "VYacg07-m82j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer = 'all-mpnet-base-v2'  # provides the best quality\n",
        "transformer = 'all-MiniLM-L6-v2'  # 5 times faster and still offers good quality"
      ],
      "metadata": {
        "id": "tgnfzgnGnMWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# automatically detect the character encoding type\n",
        "\n",
        "acceptable_confidence = .8\n",
        "\n",
        "contents = upload[input_file]\n",
        "\n",
        "codec_enc_mapping = {\n",
        "    codecs.BOM_UTF8: 'utf-8-sig',\n",
        "    codecs.BOM_UTF16: 'utf-16',\n",
        "    codecs.BOM_UTF16_BE: 'utf-16-be',\n",
        "    codecs.BOM_UTF16_LE: 'utf-16-le',\n",
        "    codecs.BOM_UTF32: 'utf-32',\n",
        "    codecs.BOM_UTF32_BE: 'utf-32-be',\n",
        "    codecs.BOM_UTF32_LE: 'utf-32-le',\n",
        "}\n",
        "\n",
        "encoding_type = 'utf-8'  # Default assumption\n",
        "is_unicode = False\n",
        "\n",
        "for bom, enc in codec_enc_mapping.items():\n",
        "    if contents.startswith(bom):\n",
        "        encoding_type = enc\n",
        "        is_unicode = True\n",
        "        break\n",
        "\n",
        "if not is_unicode:\n",
        "    # Didn't find BOM, so let's try to detect the encoding\n",
        "    guess = chardet.detect(contents)\n",
        "    if guess['confidence'] >= acceptable_confidence:\n",
        "        encoding_type = guess['encoding']\n",
        "\n",
        "print(\"Character Encoding Type Detected\", encoding_type)"
      ],
      "metadata": {
        "id": "irj1dFn42ayO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# automatically detect the delimiter\n",
        "with open(input_file,encoding=encoding_type) as myfile:\n",
        "    firstline = myfile.readline()\n",
        "myfile.close()\n",
        "delimiter_type = detect(firstline)"
      ],
      "metadata": {
        "id": "IoYTjbeCUfvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe using the detected delimiter and encoding type\n",
        "df = pd.read_csv((input_file), on_bad_lines='skip', encoding=encoding_type, delimiter=delimiter_type)\n",
        "count_rows = len(df)\n",
        "if count_rows > 50_000:\n",
        "  print(\"WARNING: You May Experience Crashes When Processing Over 50,000 Keywords at Once. Please consider smaller batches!\")\n",
        "print(\"Uploaded Keyword CSV File Successfully!\")\n",
        "df"
      ],
      "metadata": {
        "id": "5YPJV-9eoY5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardise the keyword columns\n",
        "df.rename(columns={\"Search term\": \"Keyword\", \"keyword\": \"Keyword\", \"query\": \"Keyword\", \"query\": \"Keyword\", \"Top queries\": \"Keyword\", \"queries\": \"Keyword\", \"Keywords\": \"Keyword\",\"keywords\": \"Keyword\", \"Search terms report\": \"Keyword\"}, inplace=True)\n",
        "\n",
        "if \"Keyword\" not in df.columns:\n",
        "  print(\"Error! Please make sure your csv file contains a column named 'Keyword!\")\n",
        "\n",
        "# store the data\n",
        "cluster_name_list = []\n",
        "corpus_sentences_list = []\n",
        "df_all = []\n",
        "\n",
        "corpus_set = set(df['Keyword'])\n",
        "corpus_set_all = corpus_set\n",
        "cluster = True\n"
      ],
      "metadata": {
        "id": "KhzhF3Rk_llo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering Keywords - This can take a while!"
      ],
      "metadata": {
        "id": "8yHMcc5FGZnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keep looping through until no more clusters are created\n",
        "\n",
        "cluster_accuracy = cluster_accuracy / 100\n",
        "model = SentenceTransformer(transformer)\n",
        "\n",
        "while cluster:\n",
        "\n",
        "    corpus_sentences = list(corpus_set)\n",
        "    check_len = len(corpus_sentences)\n",
        "\n",
        "    corpus_embeddings = model.encode(corpus_sentences, batch_size=256, show_progress_bar=True, convert_to_tensor=True)\n",
        "    clusters = util.community_detection(corpus_embeddings, min_community_size=min_cluster_size, threshold=cluster_accuracy, init_max_size=len(corpus_embeddings))\n",
        "\n",
        "    for keyword, cluster in enumerate(clusters):\n",
        "        print(\"\\nCluster {}, #{} Elements \".format(keyword + 1, len(cluster)))\n",
        "\n",
        "        for sentence_id in cluster[0:]:\n",
        "            print(\"\\t\", corpus_sentences[sentence_id])\n",
        "            corpus_sentences_list.append(corpus_sentences[sentence_id])\n",
        "            cluster_name_list.append(\"Cluster {}, #{} Elements \".format(keyword + 1, len(cluster)))\n",
        "\n",
        "    df_new = pd.DataFrame(None)\n",
        "    df_new['Cluster Name'] = cluster_name_list\n",
        "    df_new[\"Keyword\"] = corpus_sentences_list\n",
        "\n",
        "    df_all.append(df_new)\n",
        "    have = set(df_new[\"Keyword\"])\n",
        "\n",
        "    corpus_set = corpus_set_all - have\n",
        "    remaining = len(corpus_set)\n",
        "    print(\"Total Unclustered Keywords: \", remaining)\n",
        "    if check_len == remaining:\n",
        "        break"
      ],
      "metadata": {
        "id": "ivDBFFPipZMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a new dataframe from the list of dataframe and merge back into the orginal df\n",
        "df_new = pd.concat(df_all)\n",
        "df = df.merge(df_new.drop_duplicates('Keyword'), how='left', on=\"Keyword\")"
      ],
      "metadata": {
        "id": "BbUja0QjqZok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the clusters to the shortest keyword in the cluster\n",
        "df['Length'] = df['Keyword'].astype(str).map(len)\n",
        "df = df.sort_values(by=\"Length\", ascending=True)\n",
        "\n",
        "df['Cluster Name'] = df.groupby('Cluster Name')['Keyword'].transform('first')\n",
        "df.sort_values(['Cluster Name', \"Keyword\"], ascending=[True, True], inplace=True)\n",
        "\n",
        "df['Cluster Name'] = df['Cluster Name'].fillna(\"zzz_no_cluster\")\n",
        "\n",
        "del df['Length']"
      ],
      "metadata": {
        "id": "NL2w0FH_qvEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move the cluster and keyword columns to the front\n",
        "col = df.pop(\"Keyword\")\n",
        "df.insert(0, col.name, col)\n",
        "\n",
        "col = df.pop('Cluster Name')\n",
        "df.insert(0, col.name, col)\n",
        "\n",
        "df.sort_values([\"Cluster Name\", \"Keyword\"], ascending=[True, True], inplace=True)"
      ],
      "metadata": {
        "id": "VqIoJb-Nq2qL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uncluster_percent = (remaining / count_rows) * 100\n",
        "clustered_percent = 100 - uncluster_percent\n",
        "print(clustered_percent,\"% of rows clustered successfully!\")"
      ],
      "metadata": {
        "id": "6AluvS4eUuwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Your Keywords Clustered.csv', index=False)\n",
        "files.download(\"Your Keywords Clustered.csv\")"
      ],
      "metadata": {
        "id": "dDZQNzMPonaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}